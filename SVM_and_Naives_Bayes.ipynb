{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef1qjI_95aih"
   },
   "source": [
    "Question 1 :  What is Information Gain, and how is it used in Decision Trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHg27AaE57H1"
   },
   "source": [
    "Information Gain is a metric that measures how much a feature reduces the uncertainty (entropy) of the target variable when it is used to split the data.\n",
    "​\n",
    "\n",
    "Definition\n",
    "\n",
    "* Information Gain is defined as the reduction in entropy of the dataset after splitting on a particular attribute.\n",
    "\n",
    "* Mathematically, for dataset  T and attribute  a:  IG ( T , a ) = H ( T ) − H ( T ∣ a ) , where  H ( T ) is the entropy before the split and  H ( T ∣ a ) is the conditional entropy after splitting by attribute  a.\n",
    "\n",
    "Entropy and Intuition\n",
    "\n",
    "* Entropy H(T) measures the impurity or randomness of class labels in a dataset; higher entropy means more mixed classes, lower entropy means purer classes.\n",
    "​\n",
    "\n",
    "* A split that creates child nodes with purer class distributions (lower entropy) yields higher Information Gain, meaning the feature is more informative for classification.\n",
    "\n",
    "Use in Decision Trees\n",
    "\n",
    "* In decision tree algorithms such as ID3 and C4.5, Information Gain is used as a splitting criterion at each node. For every candidate attribute, the algorithm computes its Information Gain with respect to the current node’s data.\n",
    "​\n",
    "\n",
    "* The attribute with the maximum Information Gain is chosen to split the node, and this process is repeated recursively on each child node until stopping conditions are met (e.g., pure nodes, depth limit).\n",
    "\n",
    "​Practical Role\n",
    "\n",
    "* At the root node, the feature with the highest Information Gain becomes the root split, creating branches that best separate the classes initially.\n",
    "​\n",
    "\n",
    "* This greedy selection using Information Gain guides the tree to shorter, more discriminative paths, improving classification performance, although it can be biased toward attributes with many distinct values (motivation for Gain Ratio in C4.5).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jRz30Fv7lSt"
   },
   "source": [
    "Question 2: What is the difference between Gini Impurity and Entropy?\n",
    "\n",
    "Hint: Directly compares the two main impurity measures, highlighting strengths,\n",
    "weaknesses, and appropriate use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TdQseqm7_l8"
   },
   "source": [
    "Gini Impurity and Entropy serve as impurity measures in decision trees to assess node purity and select optimal splits, but they differ in formulas, computation, and behavior.\n",
    "\n",
    "Gini Impurity calculates as 1−∑pi2\n",
    " , where\n",
    "p\n",
    "i\n",
    "  is class probability, ranging from 0 (pure node) to 0.5 (max impurity for binary classes). Entropy computes as\n",
    "−\n",
    "∑\n",
    "p\n",
    "i\n",
    "log\n",
    "⁡\n",
    "2\n",
    "(\n",
    "p\n",
    "i\n",
    ")\n",
    ", ranging from 0 to 1, rooted in information theory for measuring uncertainty.\n",
    "\n",
    "Computation Differences\n",
    "\n",
    "Gini requires simpler squaring operations, making it faster without logarithms, ideal for large datasets. Entropy involves logarithmic terms, slowing computation but providing finer sensitivity to probability changes.\n",
    "\n",
    "Strengths and Weaknesses\n",
    "\n",
    "Gini offers efficiency as the default in CART and scikit-learn, favoring quicker splits toward dominant classes but potentially less effective on imbalanced data. Entropy, used in ID3/C4.5, yields theoretically sound splits and balanced trees yet demands more processing power.\n",
    "​\n",
    "\n",
    "Use Cases\n",
    "\n",
    "Gini suits high-dimensional or large-scale training like random forests for speed. Entropy fits smaller, balanced datasets needing precise, information-based splits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcPLjXtV88YR"
   },
   "source": [
    "Question 3:What is Pre-Pruning in Decision Trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DghifLJ9BBa"
   },
   "source": [
    "Pre-Pruning in decision trees is a technique that stops tree growth early during construction to prevent overfitting by applying predefined stopping criteria at each potential split.\n",
    "​\n",
    "\n",
    "Definition and Purpose\n",
    "Pre-Pruning, also called early stopping, halts the recursive splitting process before the tree becomes fully grown, avoiding complex structures that memorize training data noise. It uses heuristics to check conditions like minimum impurity decrease or sample size before creating child nodes.\n",
    "​\n",
    "\n",
    "Common Techniques\n",
    "\n",
    "* Maximum tree depth limits overall height, preventing deep, overly specific branches.\n",
    "​\n",
    "\n",
    "* Minimum samples per split or leaf ensures nodes have sufficient data, discarding trivial splits.\n",
    "​\n",
    "​\n",
    "\n",
    "* Minimum impurity decrease requires splits to reduce Gini or entropy by a threshold amount.\n",
    "​\n",
    "\n",
    "Advantages and Risks\n",
    "\n",
    "Pre-Pruning is computationally efficient since it avoids building then trimming a full tree, making it suitable for large datasets. However, it risks underfitting by stopping too early, missing potentially beneficial deeper splits (known as the horizon effect).\n",
    "​\n",
    "\n",
    "Comparison to Post-Pruning\n",
    "\n",
    "Unlike post-pruning, which trims a complete tree afterward, pre-pruning keeps trees smaller from the start and integrates directly into algorithms like scikit-learn via parameters such as max_depth or min_samples_split.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_mRD9XC9aQR"
   },
   "source": [
    "Question 4:Write a Python program to train a Decision Tree Classifier using Gini\n",
    "Impurity as the criterion and print the feature importances (practical).\n",
    "\n",
    "Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_.\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wrVD7UV59nrT",
    "outputId": "2d0a3dcd-aeb0-47e2-ebee-7b249e561058"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances (Gini-based):\n",
      "sepal length (cm): 0.0000\n",
      "sepal width (cm): 0.0191\n",
      "petal length (cm): 0.8933\n",
      "petal width (cm): 0.0876\n",
      "\n",
      "Model Accuracy on Test Set: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Load the Iris dataset (standard classification dataset)\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Decision Tree Classifier using Gini Impurity criterion\n",
    "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Print feature importances\n",
    "print(\"Feature Importances (Gini-based):\")\n",
    "for i, importance in enumerate(clf.feature_importances_):\n",
    "    print(f\"{feature_names[i]}: {importance:.4f}\")\n",
    "\n",
    "print(\"\\nModel Accuracy on Test Set:\", clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCUBawFY9wmP"
   },
   "source": [
    "Question 5: What is a Support Vector Machine (SVM)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BC5jeCrw96mg"
   },
   "source": [
    "Support Vector Machine (SVM) is a supervised machine learning algorithm that finds the optimal hyperplane to separate data points of different classes while maximizing the margin between them.\n",
    "​\n",
    "\n",
    "Core Concept\n",
    "SVM constructs a decision boundary (hyperplane) in feature space that best divides classes, with support vectors being the closest points to this boundary that define its position. The algorithm prioritizes the widest possible margin for better generalization and lower overfitting risk.\n",
    "​\n",
    "\n",
    "Key Components\n",
    "\n",
    "* Hyperplane: The separating line (2D), plane (3D), or higher-dimensional equivalent represented as w⋅x+b=0.\n",
    "​\n",
    "\n",
    "* Margin: Distance from hyperplane to nearest support vectors, maximized during training for robustness.\n",
    "​\n",
    "\n",
    "* Support Vectors: Critical training points lying on margin boundaries; only these influence the model.\n",
    "​\n",
    "\n",
    "Handling Nonlinear Data\n",
    "\n",
    "For non-linearly separable data, SVM uses kernel tricks (linear, polynomial, RBF) to map data into higher-dimensional space where linear separation becomes possible without explicit transformation. This enables effective classification of complex patterns like in image or text data.\n",
    "​\n",
    "\n",
    "Applications and Strengths\n",
    "\n",
    "SVM excels in high-dimensional spaces, binary classification, and scenarios with clear margins, such as text categorization or bioinformatics, while being memory-efficient by relying solely on support vectors. It supports both classification (SVC) and regression (SVR) variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znKYJHaU-aF_"
   },
   "source": [
    "Question 6:  What is the Kernel Trick in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihWsyLja-j2A"
   },
   "source": [
    "The Kernel Trick in SVM enables handling non-linearly separable data by implicitly mapping input features to a higher-dimensional space where linear separation becomes possible, without explicitly computing the costly transformation.\n",
    "\n",
    "Core Mechanism\n",
    "\n",
    "The kernel trick replaces dot products in the SVM dual formulation with a kernel function\n",
    "\n",
    "K(x\n",
    "i\n",
    " ,x\n",
    "j\n",
    " )=ϕ(x\n",
    "i\n",
    " )⋅ϕ(x\n",
    "j\n",
    " ), where\n",
    "ϕ maps data to higher dimensions. This computes similarity in the transformed space efficiently, avoiding direct feature mapping that could be computationally prohibitive.\n",
    "\n",
    "Common Kernel Functions\n",
    "\n",
    "* Linear kernel: K(x,y) = x⋅y for linearly separable data.\n",
    "\n",
    "* Polynomial kernel: K(x,y)=(x⋅y+c) d for polynomial boundaries.\n",
    "\n",
    "* RBF (Gaussian) kernel: K(x,y)=exp(−γ∥x−y∥ 2 ) for complex, smooth decision surfaces.\n",
    "\n",
    "Advantages\n",
    "\n",
    "It allows SVMs to create non-linear classifiers while maintaining the max-margin property and computational efficiency, relying only on support vectors. The approach scales well for high-dimensional implicit spaces, making SVM effective for images, text, and other non-linear problems.\n",
    "​\n",
    "\n",
    "\n",
    "\n",
    "​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_i26qGnAXbr"
   },
   "source": [
    "Question 7:  Write a Python program to train two SVM classifiers with Linear and RBF\n",
    "kernels on the Wine dataset, then compare their accuracies.\n",
    "\n",
    "Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting\n",
    "on the same dataset.\n",
    "\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LicN3_HYApVK",
    "outputId": "4e631e77-9a28-4dd1-9ced-3efe4231930d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Comparison on Wine Dataset:\n",
      "Linear Kernel Accuracy: 0.9815\n",
      "RBF Kernel Accuracy:    0.7593\n",
      "Best Kernel: Linear\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "feature_names = wine.feature_names\n",
    "\n",
    "# Split data into training and test sets (same split for fair comparison)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train SVM with Linear kernel\n",
    "svm_linear = SVC(kernel='linear', random_state=42)\n",
    "svm_linear.fit(X_train, y_train)\n",
    "y_pred_linear = svm_linear.predict(X_test)\n",
    "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
    "\n",
    "# Train SVM with RBF kernel\n",
    "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "y_pred_rbf = svm_rbf.predict(X_test)\n",
    "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
    "\n",
    "# Print results\n",
    "print(\"SVM Comparison on Wine Dataset:\")\n",
    "print(f\"Linear Kernel Accuracy: {accuracy_linear:.4f}\")\n",
    "print(f\"RBF Kernel Accuracy:    {accuracy_rbf:.4f}\")\n",
    "print(f\"Best Kernel: {'RBF' if accuracy_rbf > accuracy_linear else 'Linear'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOEC82PyAz4o"
   },
   "source": [
    "Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AszhW0BoBCU0"
   },
   "source": [
    "Naive Bayes is a probabilistic supervised learning algorithm for classification that applies Bayes' Theorem to predict class labels by calculating posterior probabilities based on feature observations.\n",
    "\n",
    "Core Mechanism\n",
    "\n",
    "The classifier computes  P ( C k ∣ x ) = P ( C k ) ∏ P ( x i ∣ C k ) P ( x )  for each class  C k  , selecting the class with maximum posterior probability, where  P ( C k ) is the prior and  P ( x i ∣ C k ) are conditional likelihoods.  ​ Variants like Gaussian (continuous features), Multinomial (counts), and Bernoulli (binary) handle different data types.  \n",
    "\n",
    "Why \"Naive\"?\n",
    "It earns the \"naive\" label due to its strong assumption of conditional independence between all features given the class, meaning P(x  1  ,x  2  ∣C  k  )=P(x  1  ∣C  k  )P(x  2  ∣C  k  ).  This unrealistic simplification ignores feature correlations but enables fast, scalable computation with closed-form parameter estimation.\n",
    "​\n",
    "\n",
    "Strengths and Applications\n",
    "\n",
    "Naive Bayes trains quickly on high-dimensional data like text, excels in spam filtering, sentiment analysis, and document classification despite the naive assumption often holding approximately. It handles missing data well and performs reliably even with limited training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfsioVcoCOvX"
   },
   "source": [
    "Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
    "Bayes, and Bernoulli Naïve Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGTvUCLmCVBD"
   },
   "source": [
    "Gaussian, Multinomial, and Bernoulli Naive Bayes are variants of the Naive Bayes classifier that differ primarily in the probability distribution they assume for features.\n",
    "\n",
    "Gaussian Naive Bayes assumes continuous features follow a normal (Gaussian) distribution, using mean and variance to model\n",
    "P(x i∣y). ​ Multinomial Naive Bayes handles discrete count data (like word frequencies), assuming a multinomial distribution suitable for non-negative integers.\n",
    "​ Bernoulli Naive Bayes works with binary features (0/1 presence/absence), modeling them via Bernoulli trials and penalizing absent features.\n",
    "\n",
    "\n",
    "Key Differences\n",
    "Gaussian suits real-valued data like sensor readings or measurements (e.g., Iris dataset). Multinomial excels in text classification with term counts or TF-IDF vectors (e.g., spam detection). Bernoulli fits binary/Boolean data like bag-of-words presence (document classification) but ignores frequencies.\n",
    "\n",
    "Formulas and Computation\n",
    "\n",
    "* Gaussian: P(xi∣y)=2πσy21exp(−2σy2(xi−μy)2)\n",
    "\n",
    "* Multinomial: Uses log probabilities of counts proportional to class priors\n",
    "\n",
    "* P(xi∣y)=pyxi(1−py)1−xiP(xi∣y)=pyxi(1−py)1−xi where xi∈{0,1}xi∈{0,1}\n",
    "\n",
    "Use Cases\n",
    "\n",
    "Choose Gaussian for continuous datasets, Multinomial for frequency-based discrete data like NLP, and Bernoulli for binary feature vectors; scikit-learn implements all three for easy selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIl06YyoEpD8"
   },
   "source": [
    "Question 10:  Breast Cancer Dataset\n",
    "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
    "dataset and evaluate accuracy.\n",
    "\n",
    "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n",
    "sklearn.datasets.\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8U-zTU_3E4CY",
    "outputId": "d1d53fa1-a7a5-4b12-e048-28cecdb8a528"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes on Breast Cancer Dataset\n",
      "Dataset shape: (569, 30) (samples, features)\n",
      "Accuracy: 0.9415\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.93      0.90      0.92        63\n",
      "      benign       0.95      0.96      0.95       108\n",
      "\n",
      "    accuracy                           0.94       171\n",
      "   macro avg       0.94      0.93      0.94       171\n",
      "weighted avg       0.94      0.94      0.94       171\n",
      "\n",
      "\n",
      "Feature means by class (first 5):\n",
      "Malignant mean (feature 0): 17.431\n",
      "Benign mean (feature 0): 12.229\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "feature_names = cancer.feature_names\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gnb.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"Gaussian Naive Bayes on Breast Cancer Dataset\")\n",
    "print(f\"Dataset shape: {X.shape} (samples, features)\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=cancer.target_names))\n",
    "print(\"\\nFeature means by class (first 5):\")\n",
    "print(f\"Malignant mean (feature 0): {gnb.theta_[0][0]:.3f}\")\n",
    "print(f\"Benign mean (feature 0): {gnb.theta_[1][0]:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
