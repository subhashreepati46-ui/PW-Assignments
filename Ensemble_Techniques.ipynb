{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:  What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it."
      ],
      "metadata": {
        "id": "T_R89gVQREpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble learning is a machine-learning approach where multiple models (called base or weak learners) are trained on the same task and their predictions are combined to produce a single, usually more accurate and robust final prediction than any individual model alone. It is used for both classification and regression problems and underlies methods like Random Forests, Gradient Boosting, and many Kaggle-winning solutions.\n",
        "\n",
        "\n",
        "Key idea\n",
        "\n",
        "* The central idea is “wisdom of the crowd”: different models make different errors, so when their outputs are aggregated (by voting, averaging, or a meta-model), many of the individual errors cancel out, improving overall accuracy and stability.\n",
        "​\n",
        "\n",
        "* Ensembles aim to combine base learners that are each at least slightly better than random but diverse in their mistakes; together they form a strong learner with lower variance, reduced bias in many cases, and better generalization to unseen data.\n"
      ],
      "metadata": {
        "id": "DOfu2UzeRIuk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Bagging and Boosting?\n"
      ],
      "metadata": {
        "id": "xu1R2vTkRs3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagging and Boosting are two core ensemble learning techniques that combine multiple models to improve performance, but they differ fundamentally in approach, training process, and goals.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Core Difference\n",
        "\n",
        "\n",
        "Training Process: Bagging (Bootstrap Aggregating) builds models in parallel using random subsets of data sampled with replacement, so each model sees slightly different training examples. Boosting builds models sequentially, where each new model corrects mistakes from the prior ones by giving more weight to misclassified samples.\n",
        "\n",
        "​\n",
        "\n",
        "Core Goals: Bagging primarily reduces variance and prevents overfitting, making it ideal for unstable, high-variance learners like deep decision trees. Boosting primarily reduces bias, transforming weak learners into a strong one by iteratively improving accuracy.\n",
        "\n",
        "​\n",
        "\n",
        "Prediction Aggregation: Bagging combines outputs via simple averaging (regression) or majority voting (classification), treating all models equally. Boosting uses weighted averaging or voting, prioritizing predictions from stronger-performing models.\n",
        "\n",
        "​\n",
        "​\n",
        "\n",
        "Strengths and Risks: Bagging works well on noisy data and parallelizes easily (e.g., Random Forest). Boosting excels on structured data but can overfit if not regularized properly (e.g., AdaBoost, XGBoost).\n",
        "​\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yEwLo2IhRv1I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?"
      ],
      "metadata": {
        "id": "U3t0u-n4TuRY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bootstrap sampling is a resampling technique that creates multiple subsets of the original dataset by randomly drawing samples with replacement, meaning the same data point can appear multiple times in a single subset while others may be left out.\n",
        "\n",
        "Role in Bagging\n",
        "\n",
        "In Bagging methods like Random Forest, bootstrap sampling generates diverse training sets for each base model (typically decision trees), ensuring about 63% unique samples per subset on average.\n",
        "\n",
        "\n",
        "enefits for Random Forest\n",
        "\n",
        "* Reduces variance by training trees on varied data, preventing all models from making identical errors.\n",
        "\n",
        "* Enables parallel training since bootstrap samples are independent.\n",
        "\n",
        "* Combined predictions (via averaging or voting) yield a stable, accurate ensemble."
      ],
      "metadata": {
        "id": "lOGGnFRDT9t9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?"
      ],
      "metadata": {
        "id": "-L8qvKPeUgN_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out-of-Bag (OOB) samples are data points from the original dataset that are not included in a specific bootstrap sample used to train an individual base model (like a decision tree) in bagging-based ensembles.\n",
        "\n",
        "How OOB Works\n",
        "\n",
        "During bootstrap sampling, each tree in Random Forest trains on ~63-67% of the data (with replacement), leaving ~33-37% as OOB samples for that tree—naturally creating held-out validation sets without needing a separate test split.\n",
        "\n",
        "\n",
        "OOB Score Usage\n",
        "\n",
        "The OOB score is computed by predicting on each data point using only trees where it was OOB, then averaging these predictions' accuracy (or error) across all points; higher OOB score means better generalization. It serves as a quick, unbiased estimate of model performance, similar to cross-validation but faster and built-in for Random Forest in libraries like scikit-learn.\n",
        "​\n"
      ],
      "metadata": {
        "id": "Mb1a_i1YU5s1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n"
      ],
      "metadata": {
        "id": "TnjbAdWTVX7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature importance analysis differs significantly between a single Decision Tree and a Random Forest due to their structures and averaging effects.\n",
        "\n",
        "\n",
        "Single Decision Tree\n",
        "\n",
        "Feature importance is calculated as the total reduction in impurity (e.g., Gini or entropy) across all nodes where the feature is used for splitting, weighted by the number of samples reaching each node. Higher nodes (affecting more data) and frequent use inflate importance, but a single tree can be biased toward features with more categories or early splits.\n",
        "​\n",
        "\n",
        "Random Forest\n",
        "\n",
        "Importance is the average impurity reduction across all trees in the forest, making it more robust and less prone to individual tree biases. Random feature selection at each split (mtry) further decorrelates trees, providing stable rankings that generalize better.\n",
        "​\n",
        "\n",
        "Key Comparison\n",
        "\n",
        "Single trees offer interpretable but unstable importance (sensitive to data splits), while Random Forest delivers reliable, averaged scores ideal for feature selection in practice.\n",
        "\n"
      ],
      "metadata": {
        "id": "FXaZWJgwVjIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "\n",
        "* Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "\n",
        "* Train a Random Forest Classifier\n",
        "\n",
        "* Print the top 5 most important features based on feature importance scores.\n",
        "\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "q_w8xMQvV9Vc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpI_HoqGRD2v",
        "outputId": "5be8797f-095b-4e83-ecae-2be50ca43cb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 most important features:\n",
            "worst area: 0.1539\n",
            "worst concave points: 0.1447\n",
            "mean concave points: 0.1062\n",
            "worst radius: 0.0780\n",
            "mean concavity: 0.0680\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split into train/test (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier (100 trees)\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42, oob_score=True)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Extract feature importances and sort\n",
        "importances = rf.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]  # Descending order\n",
        "\n",
        "# Print top 5 features\n",
        "print(\"Top 5 most important features:\")\n",
        "for i in range(5):\n",
        "    idx = indices[i]\n",
        "    print(f\"{feature_names[idx]}: {importances[idx]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "\n",
        "* Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "* Evaluate its accuracy and compare with a single Decision Tree\n",
        "\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "PpsFzH63W-oY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a complete Python program to train a Bagging Classifier (using Decision Trees as base estimators) on the Iris dataset and compare its accuracy with a single Decision Tree, demonstrating variance reduction in practice.\n"
      ],
      "metadata": {
        "id": "ETGEQ6e5XSN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train/test (70/30)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_acc = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "# Bagging Classifier (50 Decision Trees)\n",
        "bag = BaggingClassifier(estimator=DecisionTreeClassifier(random_state=42),\n",
        "                        n_estimators=50, random_state=42)\n",
        "bag.fit(X_train, y_train)\n",
        "bag_pred = bag.predict(X_test)\n",
        "bag_acc = accuracy_score(y_test, bag_pred)\n",
        "\n",
        "# Results\n",
        "print(f\"Single Decision Tree Accuracy: {dt_acc:.4f}\")\n",
        "print(f\"Bagging Classifier Accuracy: {bag_acc:.4f}\")\n",
        "print(f\"Improvement: {bag_acc - dt_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9V0mbtvXUMp",
        "outputId": "ad45c943-c810-4254-9699-0a7607fbcd97"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Decision Tree Accuracy: 1.0000\n",
            "Bagging Classifier Accuracy: 1.0000\n",
            "Improvement: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "\n",
        "* Train a Random Forest Classifier\n",
        "* Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "* Print the best parameters and final accuracy\n",
        "\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "bxHLawe8Xd7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a complete Python program to train a Random Forest Classifier on the Breast Cancer dataset, tune max_depth and n_estimators using GridSearchCV (5-fold CV), and print the best parameters with final test accuracy.\n"
      ],
      "metadata": {
        "id": "-DtR61UVXsfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train/test split (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Parameter grid for tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [5, 10, None]\n",
        "}\n",
        "\n",
        "# GridSearchCV setup\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate best model on test set\n",
        "best_rf = grid_search.best_estimator_\n",
        "best_pred = best_rf.predict(X_test)\n",
        "final_acc = accuracy_score(y_test, best_pred)\n",
        "\n",
        "# Results\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best cross-validation score:\", grid_search.best_score_)\n",
        "print(\"Final test accuracy:\", final_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ql3in0_pYHf9",
        "outputId": "796a8619-674b-4256-f5d9-c91ef542815b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'max_depth': 5, 'n_estimators': 100}\n",
            "Best cross-validation score: 0.9604395604395604\n",
            "Final test accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "\n",
        "* Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "* Compare their Mean Squared Errors (MSE)\n",
        "\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "3kG5LtRoYOOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a complete Python program to train Bagging Regressor and Random Forest Regressor on the California Housing dataset (regression for house prices) and compare their Mean Squared Errors (MSE), highlighting Random Forest's edge from feature subsampling.\n"
      ],
      "metadata": {
        "id": "fln_DT6-YsBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target  # y = median house value\n",
        "\n",
        "# Train/test split (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Bagging Regressor (50 Decision Trees)\n",
        "bag_reg = BaggingRegressor(n_estimators=50, random_state=42)\n",
        "bag_reg.fit(X_train, y_train)\n",
        "bag_pred = bag_reg.predict(X_test)\n",
        "bag_mse = mean_squared_error(y_test, bag_pred)\n",
        "\n",
        "# Random Forest Regressor (50 trees)\n",
        "rf_reg = RandomForestRegressor(n_estimators=50, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "rf_pred = rf_reg.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "# Results\n",
        "print(f\"Bagging Regressor MSE: {bag_mse:.4f}\")\n",
        "print(f\"Random Forest Regressor MSE: {rf_mse:.4f}\")\n",
        "print(f\"RF Improvement over Bagging: {bag_mse - rf_mse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8taTNZpYbFq",
        "outputId": "804e0bb0-62d5-47e2-f41d-a12867f91bb4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.2573\n",
            "Random Forest Regressor MSE: 0.2573\n",
            "RF Improvement over Bagging: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "\n",
        "* Choose between Bagging or Boosting\n",
        "* Handle overfitting\n",
        "* Select base models\n",
        "* Evaluate performance using cross-validation\n",
        "* Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "tf2y7lNHY4f_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-by-Step Approach\n",
        "Choose Bagging or Boosting: Start with Bagging (e.g., Random Forest) if data is noisy or high-variance base models like deep trees risk overfitting; financial data often has outliers/noise from transactions. Switch to Boosting (e.g., XGBoost) if bias is high and sequential error-correction boosts accuracy, but monitor for overfitting in imbalanced classes like rare defaults.\n",
        "​\n",
        "\n",
        "Handle Overfitting: Use bootstrap sampling in Bagging to average diverse trees; limit max_depth, apply min_samples_split; for Boosting, early stopping, learning rate <1, subsample ratios. Regularization via OOB scores or validation curves.\n",
        "​\n",
        "\n",
        "Select Base Models: Decision Trees as default—weak individually but excel in ensembles for non-linear interactions in demographics/transactions. Test SVM/NN if needed, but trees handle mixed data best.\n",
        "​\n",
        "\n",
        "Evaluate with Cross-Validation: Stratified K-Fold CV (e.g., 5-10 folds) preserves default class imbalance; metrics like AUC-ROC, Precision-Recall over accuracy. GridSearchCV for hyperparameter tuning.\n",
        "​\n",
        "\n",
        "Justify Ensemble Improvement: Ensembles reduce variance/bias, yielding robust predictions (e.g., 5-10% AUC lift); in loans, stable risk scores minimize false positives (costly defaults missed) and false negatives (lost revenue), aiding decisions like approval limits via feature importance insights.\n",
        "\n",
        "\n",
        "Example Code: Random Forest on Synthetic Loan Data"
      ],
      "metadata": {
        "id": "TPBaj6fHZHb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "\n",
        "# Synthetic loan data (demographics + transactions) - fixed for binary balance\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "X = pd.DataFrame({\n",
        "    'age': np.random.randint(18, 70, n_samples),\n",
        "    'income': np.random.lognormal(10, 0.5, n_samples),\n",
        "    'debt_to_income': np.random.uniform(0, 0.5, n_samples),\n",
        "    'txn_count': np.random.poisson(50, n_samples),\n",
        "    'avg_txn_amt': np.random.exponential(100, n_samples)\n",
        "})\n",
        "y = np.where((X['debt_to_income'] > 0.3) | (X['txn_count'] < 30), 1, 0)  # Binary: 1=default\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42, class_weight='balanced')\n",
        "scores = cross_val_score(rf, X, y, cv=StratifiedKFold(5), scoring='roc_auc')\n",
        "\n",
        "print(\"CV AUC-ROC Scores:\", np.round(scores, 4))\n",
        "print(\"Mean CV AUC-ROC:\", np.round(scores.mean(), 4))\n",
        "print(\"Class distribution:\", np.bincount(y))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6XylNDia8Ai",
        "outputId": "2bee316f-922a-42f9-d253-80e8d92f1beb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV AUC-ROC Scores: [1. 1. 1. 1. 1.]\n",
            "Mean CV AUC-ROC: 1.0\n",
            "Class distribution: [606 394]\n"
          ]
        }
      ]
    }
  ]
}